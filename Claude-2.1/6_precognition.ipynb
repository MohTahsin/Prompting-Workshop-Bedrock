{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b010b61e-a569-43e2-97cd-e9798e79905d",
   "metadata": {},
   "source": [
    "# **Chapter 6: Precognition (Thinking Step by Step)**\n",
    "\n",
    "---\n",
    "**Lesson:**\n",
    "\n",
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to think through your answer first. \n",
    "Guess what? Claude is the same way.\n",
    "Giving Claude time to think step by step sometimes makes Claude more accurate, particularly for complex tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062f18e-2715-463a-9db5-ca7f3f749f05",
   "metadata": {},
   "source": [
    "First we will setup our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352156f5-8b7e-4d61-b93a-c04041875177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Install dependencies\n",
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n",
    "\n",
    "%pip install --quiet langchain==0.0.304\n",
    "\n",
    "#Import libraries, and set up Bedrock client\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")\n",
    "\n",
    "#Set up model inference modifiers, and ensure Claude v2 is being called from Bedrock \n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "textgen_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=inference_modifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502384f-e01e-4249-824c-5825d67f13ea",
   "metadata": {},
   "source": [
    "# **Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4ef95-74f8-4821-9af6-985608ad1fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Example 6.1 - Movie Review**\n",
    "\n",
    "In the movie review prompt below, it's clear to a human reader that the second sentence belies the first. But Claude takes the word \"unrelated\" too literally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251a59df-b2ca-4cfd-a017-a7698b2d1d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the phrases \"blew my mind\" and \"freshness and originality\", the first sentence expresses a\n",
      "very positive sentiment about the movie. However, the second sentence sarcastically suggests the\n",
      "reviewer is out of touch, implying their praise may not be reliable. Overall I would say the review\n",
      "leans positive but the second sentence introduces some skepticism or negativity. Without more\n",
      "context, it's hard to determine an overall definitive sentiment.\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "\n",
    "Human: Is this movie review sentiment positive or negative?\n",
    "\n",
    "This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since the year 1900.\n",
    "\n",
    "Assistant:\"\"\")\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56845631-6bf2-4bc8-ae8c-3b61024b389a",
   "metadata": {},
   "source": [
    "**Example 6.2 - Better movie review**\n",
    "\n",
    "To improve Claude's response, let's allow Claude to think things out first before answering. We do that by literally spelling out the steps that Claude should take in order to process and think through the prompt. \n",
    "Now, Claude's response is much better and more nuanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c504697-a819-49c2-848a-03d8d7223b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <positive-arg>\n",
      "The review says the movie \"blew my mind with its freshness and originality\", which indicates a very\n",
      "positive sentiment. This suggests the reviewer really enjoyed the originality and new ideas in the\n",
      "movie.\n",
      "</positive-arg>\n",
      "\n",
      "<negative-arg>\n",
      "The review sarcastically says the reviewer has been \"living under a rock since 1900\", implying they\n",
      "are not familiar with movies and found this one fresh only because they have not seen many others.\n",
      "This suggests the movie is not actually that original.\n",
      "</negative-arg>\n",
      "\n",
      "The positive argument is stronger here. The main statement expresses a clearly positive reaction to\n",
      "the movie. The secondary statement about living under a rock is likely intended humorously rather\n",
      "than a literal statement about the reviewer's movie knowledge. Overall this review has a positive\n",
      "sentiment.\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "\n",
    "Human: Is this review sentiment positive or negative? First write the best arguments for each side in <positive-arg> and <negative-arg> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since 1900.\n",
    "\n",
    "Assistant:\"\"\")\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24323c14-38c0-47ea-a657-21f8f2b8cbcc",
   "metadata": {},
   "source": [
    "**Example 6.3 - Blockbusters**\n",
    "\n",
    "Letting Claude think can shift Claude's answer from incorrect to correct. It's that simple in many cases where Claude makes mistakes!\n",
    "Let's go through an example where Claude's answer is incorrect to see how asking Claude to think can fix that.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8daf00-0356-4e06-853b-178bba21d6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately, I do not have enough context to definitively state whether the directors of Jaws and\n",
      "Casino Royale are citizens of the same country. Jaws was directed by Steven Spielberg, an American\n",
      "director. Casino Royale has had multiple directors over various film adaptations, and they may have\n",
      "different citizenships. Without more details on which specific Casino Royale director you're\n",
      "referring to, I cannot reliably determine if they share the same citizenship status as Spielberg.\n",
      "I'd need the name of the specific Casino Royale director you're asking about to make that\n",
      "comparison.\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "\n",
    "Human: Are both directors of Jaws and Casino Royale citizens of the same country?\n",
    "\n",
    "Assistant:\"\"\")\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0a742-ae1d-43d7-9f26-34e38c5de185",
   "metadata": {},
   "source": [
    "**Example 6.4 - Blockbusters 2**\n",
    "\n",
    "Let's fix this by asking Claude to think step by step. Instead of spelling out each step for Claude, you can also simply use those exact words and ask Claude to think step by step first.\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5a346d-a732-49ba-a99d-2744accec6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay, let's break this down step-by-step:\n",
      "\n",
      "1) The director of Jaws is Steven Spielberg. As far as I can tell, Steven Spielberg is an American\n",
      "citizen.\n",
      "\n",
      "2) The director of Casino Royale is Martin Campbell. I wasn't able to definitively confirm his\n",
      "citizenship, so I will make the assumption that Martin Campbell is a citizen of the United Kingdom.\n",
      "\n",
      "3) The United States and the United Kingdom are different countries.\n",
      "\n",
      "Therefore, based on the information I have and the assumptions made, the directors of Jaws (Steven\n",
      "Spielberg) and Casino Royale (Martin Campbell) are citizens of different countries (United States\n",
      "and United Kingdom respectively). The answer is no, they are not citizens of the same country.\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "\n",
    "Human: Are both directors of Jaws and Casino Royale citizens of the same country? Make assumptions where needed, and think step by step.\n",
    "\n",
    "Assistant:\"\"\")\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d0390-3df1-4ef1-b9ca-7265b7ce70f9",
   "metadata": {},
   "source": [
    "**Example 6.5 - Simon says**\n",
    "\n",
    "It's important to note that to some degree, \"let's think step by step\" has been pre-built into Claude's training. You will sometimes see Claude thinking step by step even when you don't ask it to, particularly if you asked it a logic or math problem. However, even then, such techniques are not failproof, and Claude thinks step by step and still gets the wrong answer. To help Claude here, we can tell Claude exactly what process it should undertake to think through the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32980de6-b247-417c-aa0b-234194e30d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay, let's go through this step-by-step:\n",
      "\n",
      "Relevant clues:\n",
      "3. Colonel Mustard was the only person in the observatory.\n",
      "5. The person with the candlestick was in the observatory.\n",
      "\n",
      "Combining these clues, Colonel Mustard was the only person in the observatory, and the person with\n",
      "the candlestick was also in the observatory. Since Colonel Mustard was the only person in the\n",
      "observatory, he must have been the one with the candlestick.\n",
      "\n",
      "Therefore, the answer is:\n",
      "(a) Yes; Colonel Mustard was in the observatory with the candlestick\n",
      "\n",
      "The key logical steps were:\n",
      "1) Identify the relevant clues\n",
      "2) Note that Colonel Mustard was alone in the observatory\n",
      "3) The person with the candlestick was also in the observatory\n",
      "4) Therefore, Colonel Mustard must have been the one with the candlestick\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "\n",
    "Human: Use the following clues to answer the following multiple-choice question, using the following procedure:\n",
    "(1) First, go through the clues one by one and consider whether the clue is potentially relevant\n",
    "(2) Second, combine the relevant clues to reason out the answer to the question\n",
    "(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\n",
    " \n",
    "Clues:\n",
    "1. Miss Scarlett was the only person in the lounge.\n",
    "2. The person with the pipe was in the kitchen.\n",
    "3. Colonel Mustard was the only person in the observatory.\n",
    "4. Professor Plum was not in the library nor the billiard room.\n",
    "5. The person with the candlestick was in the observatory.\n",
    " \n",
    "Question: Was Colonel Mustard in the observatory with the candlestick?\n",
    "(a) Yes; Colonel Mustard was in the observatory with the candlestick\n",
    "(b) No; Colonel Mustard was not in the observatory with the candlestick\n",
    "(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n",
    " \n",
    "Assistant:\"\"\")\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39fef1-843f-4f29-a2dc-425b7eea9c93",
   "metadata": {},
   "source": [
    "# **Exercises**\n",
    "\n",
    "The following exercises will need you to manipulate the prompt to get the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73c912-13d3-4e06-a7ac-a5cd46e9def3",
   "metadata": {},
   "source": [
    "**Exercise 6.1 - Classifying Emails**\n",
    "\n",
    "In this exercise, we'll be instructing Claude to sort emails into the following categories:\n",
    "(A) Pre-sale question\n",
    "(B) Broken or defective item\n",
    "(C) Billing question\n",
    "(D) Other (please explain)\n",
    "\n",
    "For the first part of the exercise, change the prompt in the cell below to make Claude output the correct classification. Your answer needs to include the letter (A - D) of the correct choice, with the parentheses, as well as the name of the category.\n",
    "\n",
    "> *Tip: Use precognition and other techniques you've learned leading up to this chapter!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c011a614-48a0-40f7-b7a7-0932c3269ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately I don't have enough context to definitively classify this email. However, based on\n",
      "the description of the issue with the Mixmaster4000 making noise, smelling smoky, and seeming like\n",
      "burning electronics, this could potentially be classified as:\n",
      "\n",
      "- A product support request - The email describes a problem with a product (Mixmaster4000) and asks\n",
      "for a replacement, indicating the need for product support.\n",
      "\n",
      "- A warranty claim - If the product is still under warranty, the issue described would likely\n",
      "qualify as a valid reason to request a replacement under the warranty coverage.\n",
      "\n",
      "- A consumer complaint - The email has a negative tone regarding the apparent faulty product and\n",
      "could be seen as a complaint about the quality or safety of the item.\n",
      "\n",
      "Without knowing more details about the product, purchase timeline, expected resolution, etc., I can\n",
      "only guess at categories based on the brief email content provided. Please let me know if you have\n",
      "any other details that could help determine a more definitive classification.\n"
     ]
    }
   ],
   "source": [
    "#Create a prompt template that has input variable(s)\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"EMAIL\"], #Change Input variables \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Please classify this email {EMAIL} \n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variable(s)\n",
    "prompt = multi_var_prompt.format(EMAIL=\"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\") #Should be classified as (B)\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "077fcfbe-45f9-4d45-91ff-e3b8399c9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately I do not have enough context to definitively classify this email. However, based on\n",
      "the content, it seems to be an informational inquiry about whether a particular appliance (the\n",
      "Mixmaster 4000) can be used for mixing paint in addition to mixing food. Without more details or\n",
      "background, I would categorize this as a general product usage question.\n"
     ]
    }
   ],
   "source": [
    "#Create a prompt template that has input variable(s)\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"EMAIL\"], #Change Input variables \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Please classify this email {EMAIL} \n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variable(s)\n",
    "prompt = multi_var_prompt.format(EMAIL=\"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\") #Should be classified as (A)\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2e5177-1cea-46aa-9d11-597dd8883c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the content and tone, I would classify this as an angry customer complaint email. The use\n",
      "of all caps and multiple punctuation indicates frustration. The customer seems to be complaining\n",
      "that they have continued to be charged monthly fees despite having cancelled their service 4 months\n",
      "ago. They are demanding an explanation for why the charges have persisted. The tone conveys anger\n",
      "and urgency around getting this billing issue resolved.\n"
     ]
    }
   ],
   "source": [
    "#Create a prompt template that has input variable(s)\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"EMAIL\"], #Change Input variables \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Please classify this email {EMAIL} \n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variable(s)\n",
    "prompt = multi_var_prompt.format(EMAIL=\"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WHAT IS GOING ON???\") #Should be classified as (C)\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9835d89-6206-4db3-adb3-0b1e7cb6c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfortunately I do not have enough context to definitively classify this email. However, based on\n",
      "the content provided, a few possibilities come to mind:\n",
      "\n",
      "- It could be someone asking for tech support or assistance using a computer, as they say \"I am not\n",
      "good with computer. Halp.\" The misspelling of \"help\" and the informal tone suggests this may be the\n",
      "case.\n",
      "\n",
      "- It could potentially be spam or some kind of phishing attempt, as the message seems vaguely\n",
      "suspicious in its randomness and lack of context.\n",
      "\n",
      "- Without more context, it's difficult to say conclusively. Additional details about the sender,\n",
      "recipient, or circumstances around the email would help determine if it's a legitimate request for\n",
      "help, a suspicious email, or something else. Based just on the content provided, those are some\n",
      "possibilities that come to mind. Let me know if you have any other details that could help classify\n",
      "the intent behind this email.\n"
     ]
    }
   ],
   "source": [
    "#Create a prompt template that has input variable(s)\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"EMAIL\"], #Change Input variables \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Please classify this email {EMAIL} \n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variable(s)\n",
    "prompt = multi_var_prompt.format(EMAIL=\"How did I get here I am not good with computer.  Halp.\") #Should be classified as (D)\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e646003-3930-4d64-a340-1b56bf5b4eb6",
   "metadata": {},
   "source": [
    "# Chapter 6 - END."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
